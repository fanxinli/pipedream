#!/usr/bin/env python
import logging
import argparse
import importlib
import warnings
from ast import literal_eval

from collections import OrderedDict
import os
import sys

import torch
torch.manual_seed(0)
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.optim
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset
from torch.utils.data.distributed import DistributedSampler
import torchvision.transforms as transforms

from modeling import BertForPreTraining, BertConfig, CrossEntropyWrapper
from tqdm import tqdm, trange
from utils import is_main_process, format_step
import h5py
import numpy as np

def parse_args():
    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')
    # data
    dataset = parser.add_argument_group('data setup')
    dataset.add_argument('--data_dir', default='data/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/bookscorpus/',
                         help='path to directory with training/validation data')
    dataset.add_argument('--checkpoint_path', required=True,
                         help='full path to the model checkpoint file')
    # parameters
    params = parser.add_argument_group('inference setup')
    params.add_argument('--batch-size', default=8, type=int,
                        help='batch size per GPU')
    params.add_argument('--num_stages', default=4, type=int,
                        help='number of stages in split GNMT model')
    parser.add_argument('--max-length-train', default=128, type=int,
                        help='maximum sequence length for training')
    parser.add_argument('--min-length-train', default=0, type=int,
                        help='minimum sequence length for training')
    parser.add_argument("--bert_config",
                        default="bert_config.json",
                        type=str,
                        help="The BERT model config")
    parser.add_argument("--max_predictions_per_seq",
                        default=80,
                        type=int,
                        help="The maximum total of masked tokens in input sequence")

    parser.add_argument('--print-freq', '-p', default=1, type=int,
                         help='print log every PRINT_FREQ batches')
    parser.add_argument('--module', required=True,
                         help="Module to load")

    args = parser.parse_args()

    return args

class pretraining_dataset(Dataset):

    def __init__(self, input_file, max_pred_length):
        self.input_file = input_file
        self.max_pred_length = max_pred_length
        f = h5py.File(input_file, "r")
        keys = ['input_ids', 'input_mask', 'segment_ids', 'masked_lm_positions', 'masked_lm_ids',
                'next_sentence_labels']
        self.inputs = [np.asarray(f[key][:]) for key in keys]
        f.close()

    def __len__(self):
        'Denotes the total number of samples'
        return len(self.inputs[0])

    def __getitem__(self, index):

        [input_ids, input_mask, segment_ids, masked_lm_positions, masked_lm_ids, next_sentence_labels] = [
            torch.from_numpy(input[index].astype(np.int64)) if indice < 5 else torch.from_numpy(
                np.asarray(input[index].astype(np.int64))) for indice, input in enumerate(self.inputs)]

        masked_lm_labels = torch.ones(input_ids.shape, dtype=torch.long) * -1
        index = self.max_pred_length
        # store number of  masked tokens in index
        padded_mask_indices = (masked_lm_positions == 0).nonzero()
        if len(padded_mask_indices) != 0:
            index = padded_mask_indices[0].item()
        masked_lm_labels[masked_lm_positions[:index]] = masked_lm_ids[:index]

        return [input_ids, segment_ids, input_mask,
                masked_lm_labels, next_sentence_labels]

def checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.

    :param state_dict: model's state dict
    """
    ret = False
    for key, _ in state_dict.items():
        if key.find('module.') != -1:
            ret = True
            break
    return ret


def unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.

    :param state_dict: model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace('module.', '')
        new_state_dict[new_key] = value
    return new_state_dict


def get_submodule_and_parameter_name(module, parameter_name):
    parameter_name_split = parameter_name.split(".")
    submodule = module
    for attribute_name in parameter_name_split[:-1]:
        submodule = getattr(submodule, attribute_name)
    return (submodule, parameter_name_split[-1])

def main():
    args = parse_args()
    num_stages = args.num_stages

    epoch = 0
    while True:
        # no more epochs to run, since desired file not available
        if not os.path.isfile(os.path.join(args.checkpoint_path,
                                           "checkpoint.0.pth.tar.epoch."+str(epoch))):
            break

        module = importlib.import_module(args.module)
        model = module.model(None)
        num_modules = len(model)

        key_to_module_mapping = OrderedDict()
        all_stages_state_dict = OrderedDict()
        module_id = 0
        stage_id = 0
        for stage_id in range(num_stages):
            # load the checkpoint associated with a stage
            full_checkpoint_path = os.path.join(args.checkpoint_path,
                                                "checkpoint."+str(stage_id)+".pth.tar.epoch."+str(epoch))
            checkpoint = torch.load(full_checkpoint_path,
                                    map_location=torch.device('cpu'))

            # iterate through all modules in stage_id's checkpoint
            local_module_id = 0

            # quit when checkpoints for all modules in full model are loaded
            while module_id < num_modules:

                # load checkpoint corresponding to different modules in our runtime
                state_dict = checkpoint["state_dict"]
                state_dict_key = "module%d" % local_module_id

                if state_dict_key not in state_dict:
                    break
                state_dict = checkpoint["state_dict"][state_dict_key]

                # remove mask buffer
                keys_to_delete = []
                for key in state_dict:
                    if "mask" in key:
                        keys_to_delete.append(key)
                for key in keys_to_delete:
                    del state_dict[key]

                if checkpoint_from_distributed(state_dict):
                    state_dict = unwrap_distributed(state_dict)

                # collect all state_dicts in a single OrderedDict
                for key in state_dict:
                    all_stages_state_dict[(stage_id, local_module_id, key)] = state_dict[key]

                stage_module, _, _ = model[module_id]
                for key in state_dict:
                    # key_to_module_mapping maps key (in state_dict) to the
                    # torch.nn.Module wrapping the parameter and the name
                    # of parameter (weight, bias, etc.)
                    key_to_module_mapping[(stage_id, local_module_id, key)] = get_submodule_and_parameter_name(
                        stage_module, key)

                local_module_id += 1
                module_id += 1

        epoch += 1

        # Prepare model
        config = BertConfig.from_json_file(args.bert_config)

        # Padding for divisibility by 8
        if config.vocab_size % 8 != 0:
            config.vocab_size += 8 - (config.vocab_size % 8)
        model = BertForPreTraining(config)
        model_state_dict = OrderedDict()
        for real_key in model.state_dict():
            (module, parameter_name) = get_submodule_and_parameter_name(
                model, real_key)
            # find key in all_stages_state_dict that corresponds to real_key in
            # model's state_dict
            for key in key_to_module_mapping:
                (module2, parameter_name2) = key_to_module_mapping[key]
                if parameter_name == parameter_name2 and str(module) == str(module2):
                    break
            if parameter_name == parameter_name2 and str(module) == str(module2):
                model_state_dict[real_key] = all_stages_state_dict[key]
                del key_to_module_mapping[key]
                del all_stages_state_dict[key]

        # load state_dict into model, and perform inference
        model.load_state_dict(model_state_dict)
        dtype = torch.FloatTensor

        model.type(dtype)
        model.eval()

        files = [os.path.join(args.data_dir, f) for f in os.listdir(args.data_dir) if
                 os.path.isfile(os.path.join(args.data_dir, f)) and 'training' in f]
        data = pretraining_dataset(files[0], args.max_predictions_per_seq)
        sampler = RandomSampler(data)
        loader = DataLoader(data, sampler=sampler,
                            batch_size=args.batch_size, num_workers=4,
                            pin_memory=True)

        val_iter = tqdm(loader, desc="Iteration")

        num = 0
        acc = 0
        for batch in val_iter:
            input_ids, segment_ids, input_mask, _, next_sentence_labels = batch
            seq_relationship_score = model(input_ids=input_ids, token_type_ids=segment_ids,
                                                              attention_mask=input_mask, checkpoint_activations=False)
            bs = next_sentence_labels.size(0)
            prediction = torch.zeros(next_sentence_labels.size(), dtype=torch.int64)
            for i in range(bs):
                if seq_relationship_score[i][0] < seq_relationship_score[i][1]:
                    prediction[i] = 1
            num += bs
            print(seq_relationship_score)
            print(next_sentence_labels)
            print(prediction == next_sentence_labels)
            acc += (prediction == next_sentence_labels).sum().item()
            if num > 100:
                break
        print(acc/num)

if __name__ == '__main__':
    main()